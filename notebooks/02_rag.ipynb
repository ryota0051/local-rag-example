{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183c0e17-7204-4d82-9455-77b739b2e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import InfinityEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38357a71-d150-4467-8aa0-68beb200a3cb",
   "metadata": {},
   "source": [
    "## 定数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a60c569-cb12-49a2-8097-720e402dccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_PATH = 'Qwen/Qwen3-4B-Thinking-2507'\n",
    "EMBEDDING_MODEL_PATH = 'sbintuitions/sarashina-embedding-v2-1b'\n",
    "\n",
    "DB_DIR = '/app/chroma_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daecd89c-e7a2-48dd-8b24-1d621f4ca54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    検索された文書を文字列に変換\n",
    "    クエリテンプレートが含まれている場合は、元の文書内容のみを抽出\n",
    "    \"\"\"\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        content = doc.page_content\n",
    "        \n",
    "        # クエリテンプレートが含まれている場合は、元の文書内容を抽出\n",
    "        if content.startswith(\"task: クエリに関連した文章を検索してください \\\\n query: \"):\n",
    "            # クエリテンプレート部分を除去\n",
    "            original_content = content.replace(\"task: クエリに関連した文章を検索してください \\\\n query: \", \"\")\n",
    "            formatted_docs.append(original_content)\n",
    "        else:\n",
    "            formatted_docs.append(content)\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "\n",
    "def create_rag_chain(vectorstore, llm):\n",
    "    \"\"\"\n",
    "    LCEL（LangChain Expression Language）でRAGチェーンを作成\n",
    "\n",
    "    チェーンの流れ:\n",
    "    質問 → 関連文書検索 → プロンプト作成 → LLM → 文字列出力\n",
    "    \"\"\"\n",
    "    # 1. プロンプトテンプレートを作成\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"以下の文脈情報を使用して質問に答えてください。\n",
    "\n",
    "文脈情報:\n",
    "{context}\n",
    "\n",
    "質問: {question}\n",
    "\n",
    "回答:\"\"\")\n",
    "\n",
    "    # 2. 検索器（Retriever）を作成\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    # 3. LCELでRAGチェーンを構築\n",
    "    # | はパイプ演算子で、左の出力を右の入力に渡す\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs, # 質問→検索→文書をフォーマット\n",
    "            \"question\": RunnablePassthrough() # 質問をそのまま通す\n",
    "        }\n",
    "        | prompt # 辞書→プロンプトに埋め込み\n",
    "        | llm # プロンプト→LLMで処理\n",
    "    )\n",
    "\n",
    "    return rag_chain, retriever\n",
    "\n",
    "\n",
    "def simple_rag_query(question, rag_chain, retriever):\n",
    "    \"\"\"\n",
    "    LCELチェーンを使ったRAG処理\n",
    "    \"\"\"\n",
    "    print(f\"質問に関連する文書を検索中...\")\n",
    "\n",
    "    # 1. 関連文書を個別に取得（表示用）\n",
    "    docs = retriever.invoke(question)\n",
    "    print(f\"{len(docs)}件の関連文書を見つけました\")\n",
    "\n",
    "    # 2. RAGチェーンを実行\n",
    "    print(\"回答を生成中...\")\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(\"回答生成完了!!\")\n",
    "    return answer.content, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00931ab-3386-440e-be44-ca6cd794f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "質問に関連する文書を検索中...\n",
      "3件の関連文書を見つけました\n",
      "回答を生成中...\n",
      "回答生成完了!!\n",
      "CPU times: user 279 ms, sys: 122 ms, total: 401 ms\n",
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = InfinityEmbeddings(\n",
    "    model=EMBEDDING_MODEL_PATH,\n",
    "    infinity_api_url='http://proxy',\n",
    ")\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=DB_DIR,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key='EMPTY',\n",
    "    openai_api_base='http://proxy/v1',\n",
    "    model_name=CHAT_MODEL_PATH,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "rag_chain, retriever = create_rag_chain(vectorstore, llm)\n",
    "# test_question = \"task: クエリに関連した文章を検索してください \\\\n query: 生成AIとは何ですか？\"\n",
    "test_question = \"\"\"\n",
    "task: クエリに関連した文章を検索してください\n",
    "query: AI開発におけるコスト削減方法は？\n",
    "下記の例に従って回答してください。\n",
    "```\n",
    "## 朝に散歩をする\n",
    "- 理由: 脳を活性化させるため\n",
    "- 説明: 散歩をするとタスクを遂行する能力が向上する研究結果がある\n",
    "\n",
    "## 毎日早く寝る\n",
    "- 理由: 早く寝ると集中力がアップする\n",
    "- 説明: 夜更かしする人とそうでない人とでは、前者の方が集中力がアップするというデータがある\n",
    "```\n",
    "また、\n",
    "- なるべく多様な方法を紹介してください。\n",
    "- 意味的に似た内容は避けてください。\n",
    "\"\"\"\n",
    "answer, sources = simple_rag_query(test_question, rag_chain, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31bc5fb9-943c-4ecf-91d8-5a97699ecfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "task: クエリに関連した文章を検索してください \\n query: 21 \n",
      "a) 複数のリクエストを一つに集約する \n",
      "テキスト生成AI の Web API への複数リクエストを一つに集約すればコストカ\n",
      "ットにつながる可能性がある。ただしこの手法ではリクエスト 1 件あたりのト\n",
      "ークン数が多くなるため、リクエスト 1 件あたりの平均費用が高くなり、結果\n",
      "としてコストカット効果に乏しい場合もある。また、テキスト生成AIにとって\n",
      "の処理難易度が上がることで品質劣化を招く危険性もある。 \n",
      "b) テキスト生成AI以外の手法を採用する \n",
      "例えば、文中の単語や特定の品詞を抽出するだけであれば形態素解析器で十\n",
      "分であり、文間の意味的類似度を測るだけであればベクトル表現（ 埋め込み表\n",
      "現、embedding ともいう）の採用で十分な場合もある。このようにテキスト生\n",
      "成 AI 以外の手法で十分な場合ではテキスト生成 AI 以外の手法をまず検討すべ\n",
      "きである。またこれらの自然言語処理手法は、テキスト生成AIとの二者択一で\n",
      "はなく、相互に弱みを補完しながら組み合わせて利用することも十分に考えら\n",
      "れる。 \n",
      "c) 過去のテキスト生成AIの生成物を再利用する \n",
      "過去にテキスト生成AIで生成された実績のあるリクエストと同じようなリク\n",
      "エスト（例えばリクエスト文字列をベクトル表現化し、それと近しいベクトル\n",
      "表現の場合に、同じようなものとみなす）の場合、キャッシュサーバー等から\n",
      "過去の生成物を送信するようにし、テキスト生成 AI の Web APIにリクエストを\n",
      "させない。この手法をより極端にした場合、テキスト生成 AI の Web APIへのア\n",
      "クセスは全て事前にバッチ処理で行い、テキスト生成AI関連の処理にかかるコ\n",
      "ストを制御しやすくする手法も考えられる。この手法に関しては、 ５．２ ４） \n",
      "テスト済みの生成物のみを用いる場合の工夫 で詳細に述べる \n",
      "ウ テキスト生成 AI の機械学習モデルが直接提供されるケース \n",
      "大規模言語モデルの実行にともなう計算コストに直接影響される形でハード\n",
      "ウェアや運用に関連するコストが増加する。そのため大規模言語モデルの実行\n",
      "にともなう計算コストが重要であり、その観点や手法は Web API 利用のケース\n",
      "での検討事項と同様である。 \n",
      "3) 運用業務のコスト削減観点\n",
      "---\n",
      "task: クエリに関連した文章を検索してください \\n query: 20 \n",
      "当初想定した大規模言語モデルでは性能不十分であることが発覚するリスクも\n",
      "十分考えられる点にも注意する。 \n",
      "また、回答精度を上げるためにプロンプトへの記述を手厚くするほ ど、トー\n",
      "クン数や入力文字数に比例し費用が高くなるため、プロンプトの改良による精\n",
      "度改善の試行錯誤により初期検討時のプロンプトよりも費用が高くなる可能性\n",
      "も十分考えられる。想定よりもリクエスト数が多い場合に想定以上の費用が高\n",
      "くなる可能性もある。 \n",
      "この場合、初期検討時のものと比べてどこまで費用が上振れするものかのデ\n",
      "ータがまだないため、妥当な見積もり方法が確立していない点に注意が必要。 \n",
      "クラウドサービスによっては従量課金でなく定額プランを用意している場合\n",
      "もあるが、定額プランは一般的にかなり高価である。しっかりと見積も りをし\n",
      "たうえで採用することを強く推奨する。 \n",
      "ウ テキスト生成 AI の機械学習モデルが直接提供されるケース \n",
      "多くのケースで最もコストが高い選択肢になる傾向がある。少なくてもハー\n",
      "ドウェアの調達から必要な場合は、初期投資が最も高い。他の利用形態で要件\n",
      "を十分満たすかの検討とコスト試算を行ったうえで、この利用形態を採用する\n",
      "かを検討する。 \n",
      "2) アプリケーションのコスト削減観点 \n",
      "ア  テキスト生成 AI を組み込んだサービスとして提供されるケース \n",
      "テキスト生成AIで扱われる大規模言語モデルは、ひとつのモデルだけで多様\n",
      "な自然言語処理（例えば、翻訳や要約、文章作成や固有表現抽出等）が可能で\n",
      "ある。この特徴から、バラバラに実装・提供されていた複数の自然言語処理関\n",
      "連の機能や製品をまとめられないかの検討をし、重複投資を避ける。 \n",
      "イ テキスト生成 AI を利活用するための Web API がクラウドサービスと\n",
      "して提供されるケース \n",
      "生成AIへのリクエスト1件あたりの平均費用を下げるか、そもそもリクエス\n",
      "ト量を減らすことでコスト削減が可能である。平均費用の方は ３．１  １） \n",
      "ハードウェア ・ソフトウェアのコスト削減観点 で述べたため、ここでは生成\n",
      "AIへのリクエスト量を減らすための検討事項を紹介する。 \n",
      "リクエスト量を大幅に減らすことのできる選択肢として主なものは以下の 3\n",
      "つである。\n",
      "---\n",
      "task: クエリに関連した文章を検索してください \\n query: 47 \n",
      "済む方法の一つに LangChain 等の開発フレームワークの導入も考えられる。し\n",
      "かしフレームワークを導入してしまうと、今度はそのフレームワークにロック\n",
      "インされるリスクが生じる。まだ発展途上かつ発展が早い分野であるため、情\n",
      "報システムの開発体制やエンジニアのスキル等も考慮 しつつ、フレームワーク\n",
      "の安易な導入といった早すぎる最適化をしないよう慎重に検討することを推奨\n",
      "する。 \n",
      "軽減策 \n",
      "テキスト生成AI固有でない部分、例えばソフトウェア設計時にテキスト生成\n",
      "AI サービスを利用するコンポーネントを適切に分離し、そこだけ変更した場合\n",
      "の評価実験を行いやすくする等のマイクロサービス一般の工夫については割愛\n",
      "する。 \n",
      "テキスト生成AI固有の部分に関しては、基本的に十分にテストができれば問\n",
      "題ないため、７．１ ４） アップデート対応に関するリスク  と同じ内容で\n",
      "ある。新しいクラウドサービスプロバイダのテキスト生成AIサービスを利用す\n",
      "る場合のリスク全般は本章の他の節で記載されている。 \n",
      "7.2.6 コストマネジメントに関するリスク \n",
      "大規模言語モデルをクラウドサービスプロバイダの Web API で利用する場合、\n",
      "その料金形態が独特である点に注意が必要である。これは大規模言語モデルの\n",
      "推論時に、入力のプロンプトや出力される生成物の文書量のサイズが増えると\n",
      "ともに、必要とされる計算リソースが増える傾向がある性質に由来していると\n",
      "考えられる。入力のプロンプトよりも生成物の文書の方がサイズによる計算リ\n",
      "ソースの増加影響が大きい。そのため、多くの場合、入力分のサイズ（一般的\n",
      "にはトークン数、一部文字列長のケースもある）と出力文のサイズのそれぞれ\n",
      "に比例した課金形態となっている。この料金形態によりリクエスト数だけでな\n",
      "く、リクエストの中身やその結果の出力にも大きく費用が依存する形になり、\n",
      "コスト予測が難しくなっている。そのため見積もりミスによる予算超過や利用\n",
      "停止のリスクが起こりうる。 \n",
      "軽減策 \n",
      "(1) 見積もりの精度を上げる \n",
      "事前テストの結果等から入出力のテキストトークン数や文字数の悲観予測を\n",
      "しておく。トークン数の計算方法は各クラウドプロバイダーから提供されてい\n",
      "る場合も多く事前に確認しておく。 \n",
      "(2) 事前にテスト済みのものしか返さない\n"
     ]
    }
   ],
   "source": [
    "for source in sources:\n",
    "    print('---')\n",
    "    print(source.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd22424-4973-4431-9f76-de88f9e33cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## 複数のリクエストを一つに集約する\n",
      "- 理由: リクエスト数を減らすことでAPIコストを削減できる\n",
      "- 説明: 1つのリクエストで複数のタスクを処理することで、Web APIへの呼び出し回数を削減する。ただし、1件あたりのトークン数が増える可能性があるため、適切なバランスが必要。\n",
      "\n",
      "## テキスト生成AI以外の手法を採用する\n",
      "- 理由: 生成AIのコストを回避できる\n",
      "- 説明: 例えば形態素解析やベクトル表現を用いて特定のタスクを処理する場合、テキスト生成AIの利用を避け、コストを削減できる。\n",
      "\n",
      "## 過去の生成物を再利用する\n",
      "- 理由: 既存の出力を利用することでAPI呼び出しを削減\n",
      "- 説明: 類似のリクエストに対して過去の生成物をキャッシュサーバーから取得し、再生成を避けることでコストを削減できる。\n",
      "\n",
      "## 事前テストによるトークン数の予測\n",
      "- 理由: 予測誤差を防ぐことで予算超過を避ける\n",
      "- 説明: APIの入出力トークン数を事前にテストし、悲観的な見積もりをもとにコストを管理することで、実際の利用コストの上振れを抑制できる。\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c3056-7421-4f5e-b16d-474d2dbcd614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
