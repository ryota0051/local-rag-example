{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "183c0e17-7204-4d82-9455-77b739b2e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import InfinityEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38357a71-d150-4467-8aa0-68beb200a3cb",
   "metadata": {},
   "source": [
    "## 定数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a60c569-cb12-49a2-8097-720e402dccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_PATH = 'Qwen/Qwen3-4B-Thinking-2507'\n",
    "EMBEDDING_MODEL_PATH = 'sbintuitions/sarashina-embedding-v2-1b'\n",
    "\n",
    "DB_DIR = '/app/chroma_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daecd89c-e7a2-48dd-8b24-1d621f4ca54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    検索された文書を文字列に変換\n",
    "    クエリテンプレートが含まれている場合は、元の文書内容のみを抽出\n",
    "    \"\"\"\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        content = doc.page_content\n",
    "        \n",
    "        # クエリテンプレートが含まれている場合は、元の文書内容を抽出\n",
    "        if content.startswith(\"text: \"):\n",
    "            # クエリテンプレート部分を除去\n",
    "            original_content = content.replace(\"text: \", \"\")\n",
    "            formatted_docs.append(original_content)\n",
    "        else:\n",
    "            formatted_docs.append(content)\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "\n",
    "def create_rag_chain(vectorstore, llm):\n",
    "    \"\"\"\n",
    "    LCEL（LangChain Expression Language）でRAGチェーンを作成\n",
    "\n",
    "    チェーンの流れ:\n",
    "    質問 → 関連文書検索 → プロンプト作成 → LLM → 文字列出力\n",
    "    \"\"\"\n",
    "    # 1. プロンプトテンプレートを作成\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"以下の文脈情報を使用して質問に答えてください。\n",
    "\n",
    "文脈情報:\n",
    "{context}\n",
    "\n",
    "質問: {question}\n",
    "\n",
    "回答:\"\"\")\n",
    "\n",
    "    # 2. 検索器（Retriever）を作成\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    # 3. LCELでRAGチェーンを構築\n",
    "    # | はパイプ演算子で、左の出力を右の入力に渡す\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs, # 質問→検索→文書をフォーマット\n",
    "            \"question\": RunnablePassthrough() # 質問をそのまま通す\n",
    "        }\n",
    "        | prompt # 辞書→プロンプトに埋め込み\n",
    "        | llm # プロンプト→LLMで処理\n",
    "    )\n",
    "\n",
    "    return rag_chain, retriever\n",
    "\n",
    "\n",
    "def simple_rag_query(question, rag_chain, retriever):\n",
    "    \"\"\"\n",
    "    LCELチェーンを使ったRAG処理\n",
    "    \"\"\"\n",
    "    print(f\"質問に関連する文書を検索中...\")\n",
    "\n",
    "    # 1. 関連文書を個別に取得（表示用）\n",
    "    docs = retriever.invoke(question)\n",
    "    print(f\"{len(docs)}件の関連文書を見つけました\")\n",
    "\n",
    "    # 2. RAGチェーンを実行\n",
    "    print(\"回答を生成中...\")\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(\"回答生成完了!!\")\n",
    "    return answer.content, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f00931ab-3386-440e-be44-ca6cd794f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "質問に関連する文書を検索中...\n",
      "3件の関連文書を見つけました\n",
      "回答を生成中...\n",
      "回答生成完了!!\n",
      "CPU times: user 103 ms, sys: 118 ms, total: 221 ms\n",
      "Wall time: 40.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = InfinityEmbeddings(\n",
    "    model=EMBEDDING_MODEL_PATH,\n",
    "    infinity_api_url='http://proxy',\n",
    ")\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=DB_DIR,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key='EMPTY',\n",
    "    openai_api_base='http://proxy/v1',\n",
    "    model_name=CHAT_MODEL_PATH,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "rag_chain, retriever = create_rag_chain(vectorstore, llm)\n",
    "# test_question = \"task: クエリに関連した文章を検索してください \\\\n query: 生成AIとは何ですか？\"\n",
    "test_question = \"\"\"\n",
    "task: クエリに関連した文章を検索してください\n",
    "query: AI開発におけるコスト削減方法は？\n",
    "下記の例に従って回答してください。\n",
    "```\n",
    "## 朝に散歩をする\n",
    "- 理由: 脳を活性化させるため\n",
    "- 説明: 散歩をするとタスクを遂行する能力が向上する研究結果がある\n",
    "\n",
    "## 毎日早く寝る\n",
    "- 理由: 早く寝ると集中力がアップする\n",
    "- 説明: 夜更かしする人とそうでない人とでは、前者の方が集中力がアップするというデータがある\n",
    "```\n",
    "また、\n",
    "- なるべく多様な方法を紹介してください。\n",
    "- 意味的に似た内容は避けてください。\n",
    "\"\"\"\n",
    "answer, sources = simple_rag_query(test_question, rag_chain, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31bc5fb9-943c-4ecf-91d8-5a97699ecfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "text: 21 \n",
      "a) 複数のリクエストを一つに集約する \n",
      "テキスト生成AI の Web API への複数リクエストを一つに集約すればコストカ\n",
      "ットにつながる可能性がある。ただしこの手法ではリクエスト 1 件あたりのト\n",
      "ークン数が多くなるため、リクエスト 1 件あたりの平均費用が高くなり、結果\n",
      "としてコストカット効果に乏しい場合もある。また、テキスト生成AIにとって\n",
      "の処理難易度が上がることで品質劣化を招く危険性もある。 \n",
      "b) テキスト生成AI以外の手法を採用する \n",
      "例えば、文中の単語や特定の品詞を抽出するだけであれば形態素解析器で十\n",
      "分であり、文間の意味的類似度を測るだけであればベクトル表現（ 埋め込み表\n",
      "現、embedding ともいう）の採用で十分な場合もある。このようにテキスト生\n",
      "成 AI 以外の手法で十分な場合ではテキスト生成 AI 以外の手法をまず検討すべ\n",
      "きである。またこれらの自然言語処理手法は、テキスト生成AIとの二者択一で\n",
      "はなく、相互に弱みを補完しながら組み合わせて利用することも十分に考えら\n",
      "れる。 \n",
      "c) 過去のテキスト生成AIの生成物を再利用する \n",
      "過去にテキスト生成AIで生成された実績のあるリクエストと同じようなリク\n",
      "エスト（例えばリクエスト文字列をベクトル表現化し、それと近しいベクトル\n",
      "表現の場合に、同じようなものとみなす）の場合、キャッシュサーバー等から\n",
      "過去の生成物を送信するようにし、テキスト生成 AI の Web APIにリクエストを\n",
      "させない。この手法をより極端にした場合、テキスト生成 AI の Web APIへのア\n",
      "クセスは全て事前にバッチ処理で行い、テキスト生成AI関連の処理にかかるコ\n",
      "ストを制御しやすくする手法も考えられる。この手法に関しては、 ５．２ ４） \n",
      "テスト済みの生成物のみを用いる場合の工夫 で詳細に述べる \n",
      "ウ テキスト生成 AI の機械学習モデルが直接提供されるケース \n",
      "大規模言語モデルの実行にともなう計算コストに直接影響される形でハード\n",
      "ウェアや運用に関連するコストが増加する。そのため大規模言語モデルの実行\n",
      "にともなう計算コストが重要であり、その観点や手法は Web API 利用のケース\n",
      "での検討事項と同様である。 \n",
      "3) 運用業務のコスト削減観点\n",
      "---\n",
      "text: 20 \n",
      "当初想定した大規模言語モデルでは性能不十分であることが発覚するリスクも\n",
      "十分考えられる点にも注意する。 \n",
      "また、回答精度を上げるためにプロンプトへの記述を手厚くするほ ど、トー\n",
      "クン数や入力文字数に比例し費用が高くなるため、プロンプトの改良による精\n",
      "度改善の試行錯誤により初期検討時のプロンプトよりも費用が高くなる可能性\n",
      "も十分考えられる。想定よりもリクエスト数が多い場合に想定以上の費用が高\n",
      "くなる可能性もある。 \n",
      "この場合、初期検討時のものと比べてどこまで費用が上振れするものかのデ\n",
      "ータがまだないため、妥当な見積もり方法が確立していない点に注意が必要。 \n",
      "クラウドサービスによっては従量課金でなく定額プランを用意している場合\n",
      "もあるが、定額プランは一般的にかなり高価である。しっかりと見積も りをし\n",
      "たうえで採用することを強く推奨する。 \n",
      "ウ テキスト生成 AI の機械学習モデルが直接提供されるケース \n",
      "多くのケースで最もコストが高い選択肢になる傾向がある。少なくてもハー\n",
      "ドウェアの調達から必要な場合は、初期投資が最も高い。他の利用形態で要件\n",
      "を十分満たすかの検討とコスト試算を行ったうえで、この利用形態を採用する\n",
      "かを検討する。 \n",
      "2) アプリケーションのコスト削減観点 \n",
      "ア  テキスト生成 AI を組み込んだサービスとして提供されるケース \n",
      "テキスト生成AIで扱われる大規模言語モデルは、ひとつのモデルだけで多様\n",
      "な自然言語処理（例えば、翻訳や要約、文章作成や固有表現抽出等）が可能で\n",
      "ある。この特徴から、バラバラに実装・提供されていた複数の自然言語処理関\n",
      "連の機能や製品をまとめられないかの検討をし、重複投資を避ける。 \n",
      "イ テキスト生成 AI を利活用するための Web API がクラウドサービスと\n",
      "して提供されるケース \n",
      "生成AIへのリクエスト1件あたりの平均費用を下げるか、そもそもリクエス\n",
      "ト量を減らすことでコスト削減が可能である。平均費用の方は ３．１  １） \n",
      "ハードウェア ・ソフトウェアのコスト削減観点 で述べたため、ここでは生成\n",
      "AIへのリクエスト量を減らすための検討事項を紹介する。 \n",
      "リクエスト量を大幅に減らすことのできる選択肢として主なものは以下の 3\n",
      "つである。\n",
      "---\n",
      "text: 32 \n",
      "イアスを抑止できる。例えば、検索拡張生成  (RAG, Retrieval Augmented \n",
      "Generative) の技法、つまり文章検索で関連文章を抽出し、それを大規模言語モ\n",
      "デルにプロンプトとして渡す手法を用いる場合は \n",
      "(1) 質問文に対して適切な関連文章が抽出できているか \n",
      "(ア) もし想定していた関連文章があれば、 それをちゃんと抽出して\n",
      "いるか（再現率の観点） \n",
      "(イ) 抽出した関連文章に不適切なものが含まれていないか（適合性\n",
      "の観点） \n",
      "(2) 関連文章に対して大規模言語モデルの出力が適切か \n",
      "(ア) 出力結果に含まれている固有名詞は関連文章内に含まれている\n",
      "か \n",
      "(イ) 出力結果の内容は関連文章内で言及されているものか \n",
      "(ウ) 複数の関連文章の結果を統合している場合、その論理の繋がり\n",
      "は自然か \n",
      "(3) 最終的な生成物は求められる品質をクリアしているか \n",
      "(ア) 十分読みやすいか（ユースケースに応じてもっと観点を明確に\n",
      "する） \n",
      "(イ) 出力形式は意図通りになっているか \n",
      "等の観点や評価項目が考えられる。分解したところで、主観に基づくバイア\n",
      "スの影響は完全に除外できないが、分解しない場合と比べて大幅に抑制可能で\n",
      "ある。 \n",
      "(2) 複数人により独立に実施する \n",
      "1 つの項目に定性テストを行う人間を複数人アサインし、それぞれが独立に\n",
      "評価した内容を総合することで主観に基づくバイアスを抑止できる。いわゆる\n",
      "ダブルチェック、トリプルチェックである。 \n",
      "ただし、その複数人が同じ種類のバイアスをもっている時はリスクが軽減せ\n",
      "ず、人件費や開発リードタイムは明確に増加するといった欠点も多い。 \n",
      "ウ 費用や開発リードタイムの増加に起因するリスク \n",
      "定性テストを実施するにあたって、人員をアサインした場合にその人件費が\n",
      "増加し、また定性テスト自体も時間のかかる工程であるため開発リードタイム\n",
      "の増加を招きやすい。これはプロジェクト管理の観点から明確なリスクである。  \n",
      "軽減策 \n",
      "(1) 機械的な品質評価手法を導入する\n"
     ]
    }
   ],
   "source": [
    "for source in sources:\n",
    "    print('---')\n",
    "    print(source.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dd22424-4973-4431-9f76-de88f9e33cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## 過去の生成物をキャッシュで再利用する\n",
      "- 理由: リクエストの重複を防ぎ、Web APIへのアクセスを削減するため\n",
      "- 説明: 既に生成された実績のあるリクエストと類似のリクエストが来た場合、キャッシュサーバーから過去の生成物を再利用することで、テキスト生成AIのWeb APIにリクエストを送らない。これにより、リクエスト量が減少し、コストが削減される。\n",
      "\n",
      "## テキスト生成AI以外のNLP手法を採用する\n",
      "- 理由: テキスト生成AIのコストを回避し、効率的な処理を実現するため\n",
      "- 説明: 文中の単語や品詞の抽出には形態素解析器、意味的類似度の測定にはベクトル表現（埋め込み表現）が十分な場合がある。これらの手法を用いることで、テキスト生成AIの利用を避け、コストを削減できる。\n",
      "\n",
      "## RAG技術を導入する\n",
      "- 理由: 関連文章の抽出により、大規模言語モデルの生成リクエストを削減するため\n",
      "- 説明: 検索拡張生成（RAG）の技法では、質問文に対して関連文章を抽出し、それらをプロンプトとして大規模言語モデルに渡す。これにより、生成に必要なリクエストの数が減少し、コストが削減される。\n",
      "\n",
      "## 機械的品質評価手法を導入する\n",
      "- 理由: 人間による定性テストのコストと時間の増加を抑制するため\n",
      "- 説明: 生成物の品質を機械的な手法（例: ベクトル距離の計算や自動チェック）で評価することで、人間のチェックを必要とせず、人件費や開発リードタイムを削減できる。\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c3056-7421-4f5e-b16d-474d2dbcd614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
